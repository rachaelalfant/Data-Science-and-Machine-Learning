{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "Given a set of features $X = x_1, ..., x_m$ and a target $y \\in \\mathbb{R}^p$, the [__Multilayer Perceptron (MLP) algorithm__](https://scikit-learn.org/stable/modules/neural_networks_supervised.html) learns a function $f: \\mathbb{R}^m \\to \\mathbb{R}^p$ for the purposes of classification or regression. An MLP has an input layer, output layer, and one or more hidden layers (which may be non-linear). \n",
    "\n",
    "In this module, we build a MLP with an input layer ($L^0$) containing $784$ input nodes, 2 hidden layers ($L^1, L^2$), and $10$ output nodes ($L^{3}$).  \n",
    "\n",
    "For $l = 1, 2, 3$, layer $l$ will have two phases:\n",
    "\n",
    "* The preactivation phase $z^l = W^la^{l-1} + b^l,$ \n",
    "    - A weighted linear combination of postactivation values in the previous layer.\n",
    "* The postactivation phase $a^l = \\sigma(z^l).$ \n",
    "    - Passes the preactivation value through a chosen activation function elementwise. \n",
    "    \n",
    "We denote $a^0 = x$, where $x$ is the current input data into our network. \n",
    "\n",
    "We use the Sigmoid Function as our activation function:\n",
    "$$\n",
    "\\sigma(s) = \\frac{1}{1+e^{-s}}.\n",
    "$$\n",
    "\n",
    "We use the Mean Squared Error as our cost function:\n",
    "$$\n",
    "C = C(W, b) = \\frac{1}{2}\\sum_{i=1}^n(a^i - y^i)^2.\n",
    "$$\n",
    "\n",
    "\n",
    "## Fashion_MNIST Data Set\n",
    "\n",
    "The [Fashion MNIST dataset](https://keras.io/api/datasets/fashion_mnist/#load_data-function) consists of $60,000$ $28x28$ grayscale images of 10 fashion categories, along with a test set of $10,000$ images. Below are the labels and their corresponding descriptions:\n",
    "\n",
    "    Label:Description\n",
    "\n",
    "    0:T-shirt/top\n",
    "    1:Trouser\n",
    "    2:Pullover\n",
    "    3:Dress\n",
    "    4:Coat\n",
    "    5:Sandal\n",
    "    6:Shirt\n",
    "    7:Sneaker\n",
    "    8:Bag\n",
    "    9:Ankle boot\n",
    "    \n",
    "---\n",
    "\n",
    "The following packages are required to run the attached code:\n",
    "\n",
    "- [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
    "- [Numpy](https://numpy.org/doc/)\n",
    "- [Matplotlib.pyplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html)\n",
    "- [Keras.datasets](https://keras.io/api/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# The data structures train_X and test_X are stored as 3 dimensional tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set and testing set\n",
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()\n",
    "\n",
    "# normalize the data for training\n",
    "train_X = train_X/np.max(train_X)\n",
    "test_X = test_X/np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# flatten the training images into coloumn vectors. \n",
    "X = []\n",
    "for x in train_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "# temporarily store encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "# store the training data as a tuple\n",
    "train_data = [p for p in zip(X, Y)]\n",
    "p = train_data[0]\n",
    "print(p[1])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly with the testing data\n",
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "  temp_vec = np.zeros((10, 1))\n",
    "  temp_vec[y][0] = 1.0\n",
    "  Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our activation function\n",
    "def sigmoid(z):\n",
    "  return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1.0-sigmoid(z))\n",
    "\n",
    "# define our cost function\n",
    "def mse(a, y):\n",
    "  return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n",
    "\n",
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "  W = [[0.0]]\n",
    "  B = [[0.0]]\n",
    "  for i in range(1, len(layers)):\n",
    "    w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "    b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "\n",
    "    W.append(w_temp)\n",
    "    B.append(b_temp)\n",
    "  return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to begin calculating the gradient, we compute the output error, as well as the neuron error, as follows:\n",
    "\n",
    "#### The Output Error\n",
    "\n",
    "$$\n",
    "\\delta^{L-1} = \\nabla_{a^{L-1}}C \\otimes \\sigma'(z^{L-1}) \n",
    "$$\n",
    "\n",
    "#### The Neuron Error\n",
    "\n",
    "$$\n",
    "\\delta^{l} = ((W^{l+1})^{\\top} a^{l+1}) \\otimes \\sigma'(z^l)\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.208978368356998\n"
     ]
    }
   ],
   "source": [
    "W, B = initialize_weights(layers=[784, 60, 60, 10])\n",
    "x, y = train_data[0]\n",
    "Z = [[0.0]]\n",
    "A = [x]\n",
    "L = len(B)\n",
    "for i in range(1, L):\n",
    "  z = (W[i] @ A[i-1]) + B[i]\n",
    "  a = sigmoid(z)\n",
    "\n",
    "  Z.append(z)\n",
    "  A.append(a)\n",
    "   \n",
    "\n",
    "deltas = dict()\n",
    "delta_last = (A[-1] - y)*sigmoid_prime(Z[-1])\n",
    "deltas[L-1] = delta_last\n",
    "\n",
    "for l in range(L-2, 0, -1):\n",
    "  deltas[l] = (W[l+1].T @ deltas[l+1])*sigmoid_prime(Z[l])\n",
    "\n",
    "alpha = 0.04\n",
    "for i in range(1, 4):\n",
    "  W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "  B[i] = B[i] - alpha*deltas[i]\n",
    "    \n",
    "def forward_pass(W, B, p, predict_vector = False):\n",
    "  Z =[[0.0]]\n",
    "  A = [p[0]]\n",
    "  L = len(W)\n",
    "  for i in range(1, L):\n",
    "    z = (W[i] @ A[i-1]) + B[i]\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "\n",
    "  if predict_vector == True:\n",
    "    return A[-1]\n",
    "  else:\n",
    "    return Z, A\n",
    "\n",
    "def deltas_dict(W, B, p):\n",
    "  Z, A = forward_pass(W, B, p)\n",
    "  L = len(W)\n",
    "  deltas = dict()\n",
    "  deltas[L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "  for l in range(L-2, 0, -1):\n",
    "    deltas[l] = (W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "  return A, deltas\n",
    "\n",
    "\n",
    "def MSE(W, B, data):\n",
    "  c = 0.0\n",
    "  for p in data:\n",
    "    a = forward_pass(W, B, p, predict_vector=True)\n",
    "    c += mse(a, p[1])\n",
    "  return c/len(data)\n",
    "\n",
    "\n",
    "W, B = initialize_weights()\n",
    "print(f\"Initial Cost = {MSE(W, B, train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value = 7\n",
      "Actual Value = 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
    "print(f\"Predicted Value = {prediction}\")\n",
    "print(f\"Actual Value = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Observe that our first predicted value was incorrect - our algorithm predicted sneaker, even though the item is a shirt. Now, we will attempt to use Stochastic Gradient Descent to train our model. \n",
    "\n",
    "[__Stochastic Gradient Descent__](https://scikit-learn.org/stable/modules/sgd.html) is an iterative method used in optimization that is also utilized by machine learners to train models because it's highly efficient. We implement it for our data below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.208978368356998\n",
      "0 Cost = 0.1222268161459043\n",
      "1 Cost = 0.10698037499297447\n",
      "2 Cost = 0.09803004130197836\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gradient_descent(W, B, data, alpha = 0.04, epochs = 3):\n",
    "  L = len(W)\n",
    "  print(f\"Initial Cost = {MSE(W, B, data)}\")\n",
    "  for k in range(epochs):\n",
    "    for p in data:\n",
    "      A, deltas = deltas_dict(W, B, p)\n",
    "      for i in range(1, L):\n",
    "        W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
    "        B[i] = B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {MSE(W, B, data)}\")\n",
    "    \n",
    "    \n",
    "stochastic_gradient_descent(W, B, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Notice that our cost decreases with each iteration. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachaelalfant/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value = 1\n",
      "Actual Value = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
    "print(f\"Predicted Value = {prediction}\")\n",
    "print(f\"Actual Value = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This time, our predicted value is correct! Now, using our code from above, we define a MLP class and train our data using the Mini-batch Gradient Descent algorithm, which is similar to the Gradient Descent algorithm except that it splits the training data into small \"batches\" or subsets. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.533849394221658\n",
      "2 Cost = 0.19551198599029804\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron():\n",
    "  \n",
    "  def __init__(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W =[[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "    self.layers = layers\n",
    "    self.L = len(self.layers)\n",
    "    self.W = [[0.0]]\n",
    "    self.B = [[0.0]]\n",
    "    for i in range(1, self.L):\n",
    "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "      self.W.append(w_temp)\n",
    "      self.B.append(b_temp)\n",
    "\n",
    "\n",
    "  def forward_pass(self, p, predict_vector = False):\n",
    "    Z =[[0.0]]\n",
    "    A = [p[0]]\n",
    "    for i in range(1, self.L):\n",
    "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "      a = sigmoid(z)\n",
    "      Z.append(z)\n",
    "      A.append(a)\n",
    "\n",
    "    if predict_vector == True:\n",
    "      return A[-1]\n",
    "    else:\n",
    "      return Z, A\n",
    "\n",
    "  def MSE(self, data):\n",
    "    c = 0.0\n",
    "    for p in data:\n",
    "      a = self.forward_pass(p, predict_vector=True)\n",
    "      c += mse(a, p[1])\n",
    "    return c/len(data)\n",
    "\n",
    "  def deltas_dict(self, p):\n",
    "    Z, A = self.forward_pass(p)\n",
    "    deltas = dict()\n",
    "    deltas[self.L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
    "    for l in range(self.L-2, 0, -1):\n",
    "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
    "\n",
    "    return A, deltas\n",
    "\n",
    "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    for k in range(epochs):\n",
    "      for p in data:\n",
    "        A, deltas = self.deltas_dict(p)\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "\n",
    "  def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
    "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "    data_length = len(data)\n",
    "    for k in range(epochs):\n",
    "      for j in range(0, data_length-batch_size, batch_size):\n",
    "        delta_list = []\n",
    "        A_list = []\n",
    "        for p in data[j:j+batch_size]:\n",
    "          A, deltas = self.deltas_dict(p)\n",
    "          delta_list.append(deltas)\n",
    "          A_list.append(A)\n",
    "\n",
    "        for i in range(1, self.L):\n",
    "          self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "          self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "    \n",
    "    \n",
    "net = MultilayerPerceptron(layers=[784, 100, 100, 10])\n",
    "net.mini_batch_gradient_descent(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachaelalfant/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2446645729472044"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value = 7\n",
      "Actual Value = 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_X))\n",
    "prediction = np.argmax(net.forward_pass(test_data[i], predict_vector=True))\n",
    "print(f\"Predicted Value = {prediction}\")\n",
    "print(f\"Actual Value = {test_y[i]}\")\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Our predicted value is correct again! Although our cost decreased with each iteration again, our final cost value was lower for Stochastic Gradient Descent than for Mini-Batch Gradient Descent. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
